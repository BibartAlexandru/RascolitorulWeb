<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.KernelMemory.AI.Onnx</name>
    </assembly>
    <members>
        <member name="T:Microsoft.KernelMemory.KernelMemoryBuilderExtensions">
            <summary>
            Kernel Memory builder extensions
            </summary>
        </member>
        <member name="T:Microsoft.KernelMemory.DependencyInjection">
            <summary>
            .NET IServiceCollection dependency injection extensions.
            </summary>
        </member>
        <member name="T:Microsoft.KernelMemory.OnnxConfig.OnnxSearchType">
            <summary>
            An enum representing the possible text generation search types used by OnnxTextGenerator.
            See https://onnxruntime.ai/docs/genai/reference/config.html#search-combinations for more details.
            </summary>
        </member>
        <member name="F:Microsoft.KernelMemory.OnnxConfig.OnnxSearchType.BeamSearch">
            <summary>
            A decoding algorithm that keeps track of the top K sequences at each step. It explores
            multiple paths simultaneously, balancing exploration and exploitation. Often results in more
            coherent and higher quality text generation than Greedy Search would.
            </summary>
        </member>
        <member name="F:Microsoft.KernelMemory.OnnxConfig.OnnxSearchType.GreedySearch">
            <summary>
            The default and simplest decoding algorithm. At each step, a token is selected with the highest
            probability as the next word in the sequence.
            </summary>
        </member>
        <member name="F:Microsoft.KernelMemory.OnnxConfig.OnnxSearchType.TopN">
            <summary>
            Combined Top-P (Nucleus) and Top-K Sampling: A decoding algorithm that samples from the top k tokens
            with the highest probabilities, while also considering the smallest set of tokens whose cumulative
            probability exceeds a threshold p. This approach dynamically balances diversity and coherence in
            text generation by adjusting the sampling pool based on both fixed and cumulative probability criteria.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.TextModelDir">
            <summary>
            Path to the directory containing the .ONNX file for Text Generation.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.MaxTokens">
            <summary>
            The maximum length of the response that the model will generate. See https://onnxruntime.ai/docs/genai/reference/config.html
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.MinLength">
            <summary>
            The minimum length of the response that the model will generate. See https://onnxruntime.ai/docs/genai/reference/config.html
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.SearchType">
            <summary>
            The algorithm used in text generation. Defaults to GreedySearch.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.NumBeams">
            <summary>
            The number of beams to apply when generating the output sequence using beam search.
            If NumBeams=1, then generation is performed using greedy search. If NumBeans > 1, then
            generation is performed using beam search. A null value implies using TopN search.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.NucleusSampling">
            <summary>
            Only includes the most probable tokens with probabilities that add up to P or higher.
            Defaults to 1, which includes all of the tokens. Range is 0 to 1, exclusive of 0.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.EarlyStopping">
            <summary>
            Whether to stop the beam search when at least NumBeams sentences are finished per batch or not. Defaults to false.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.ResultsPerPrompt">
            <summary>
            The number of sequences (responses) to generate. Returns the sequences with the highest scores in order.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.TopK">
            <summary>
            Only includes tokens that fall within the list of the K most probable tokens. Range is 1 to the vocabulary size.
            Defaults to 50.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.RepetitionPenalty">
            <summary>
            Discounts the scores of previously generated tokens if set to a value greater than 1.
            Defaults to 1.
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.OnnxConfig.LengthPenalty">
            <summary>
            Controls the length of the output generated. Value less than 1 encourages the generation
            to produce shorter sequences. Values greater than 1 encourages longer sequences. Defaults to 1.
            </summary>
        </member>
        <member name="M:Microsoft.KernelMemory.OnnxConfig.Validate(System.Boolean)">
            <summary>
            Verify that the current state is valid.
            </summary>
        </member>
        <member name="T:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator">
             <summary>
             Text generator based on ONNX models, via OnnxRuntimeGenAi
             See https://github.com/microsoft/onnxruntime-genai
            
             Note: does not support model name override via request context
             </summary>
        </member>
        <member name="F:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator._model">
            <summary>
            The ONNX Model used for text generation
            </summary>
        </member>
        <member name="F:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator._tokenizer">
            <summary>
            Tokenizer used with the Onnx Generator and Model classes to produce tokens.
            This has the potential to contain a null value, depending on the contents of the Model Directory.
            </summary>
        </member>
        <member name="F:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator._textTokenizer">
            <summary>
            Tokenizer used for GetTokens() and CountTokens()
            </summary>
        </member>
        <member name="P:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator.MaxTokenTotal">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator.#ctor(Microsoft.KernelMemory.OnnxConfig,Microsoft.KernelMemory.AI.ITextTokenizer,Microsoft.Extensions.Logging.ILoggerFactory)">
            <summary>
            Create a new instance
            </summary>
            <param name="config">Configuration settings</param>
            <param name="textTokenizer">Text Tokenizer</param>
            <param name="loggerFactory">Application Logger instance</param>
        </member>
        <member name="M:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator.GenerateTextAsync(System.String,Microsoft.KernelMemory.AI.TextGenerationOptions,System.Threading.CancellationToken)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator.CountTokens(System.String)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator.GetTokens(System.String)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.KernelMemory.AI.Onnx.OnnxTextGenerator.Dispose">
            <inheritdoc/>
        </member>
    </members>
</doc>
